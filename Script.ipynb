{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and analysis of the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Program starts on daily bases by using all the log data available (min. one day / max. one month)\n",
    "The data gets analyzed and the program gives 2 csv files, which contain the important information. One is for all errors that occured in the past time and the other one for actions, country codes and so on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import csv\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "from difflib import SequenceMatcher\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA_NAME is a string of the name of the file\n",
    "DATA_TYPE is a string containing the file suffix\n",
    "DATA_START is a string (format '%Y-%m-%d %H:%M:%S,%f') containing the beginning of the period that shall be analyzed\n",
    "DATA_END is a string (format '%Y-%m-%d %H:%M:%S,%f') containing the end for analysis\n",
    "ERR_STAT is a boolean and is about whether an error_stat_analysis csv file shall be created (True) or not (False)\n",
    "NORMAL_STAT is a list containing booleans. \n",
    "                                     NORMAL_STAT[0] -> create a csv file or not\n",
    "                                     NORMAL_STAT[1] -> csv file contains the robot specific actions\n",
    "                                     NORMAL_STAT[2] -> csv file contains the robot specific country codes\n",
    "                                     NORMAL_STAT[3] -> csv file contains every action as a line for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_NAME = \"VerweildauerTest\"  \n",
    "DATA_TYPE = \".txt\"\n",
    "DATA_START = \"2019-06-01 00:00:00,001\"\n",
    "DATA_END = \"2019-06-30 23:59:59,999\"\n",
    "ERR_STAT = False\n",
    "NORMAL_STAT = [True,True, True, True]\n",
    "#'stat-2019-06'\n",
    "#\"test_time_set\"  \n",
    "#\"test_time_set_2\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checker Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checking whether the given data is valid and can be used.\n",
    "Only correct data gets processed in the following program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_bracket(): Checks whether all brackets that are opened are closed again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_bracket(json_string):\n",
    "    counter, counter2, counter3 = 0, 0, 0\n",
    "    counter += json_string.count(\"{\")\n",
    "    counter -= json_string.count(\"}\")\n",
    "    counter2 += json_string.count(\"[\")\n",
    "    counter2 -= json_string.count(\"]\")\n",
    "    counter3 += json_string.count(\"(\")\n",
    "    counter3 -= json_string.count(\")\")\n",
    "    if counter == 0 and counter2==0 and counter3 == 0:\n",
    "        return True\n",
    "    print(\"SyntaxError: Bracket\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_syntax():Checks whether a given string uses correct json / dict syntax or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_syntax(json_string):\n",
    "    try:\n",
    "        json_str = json.loads(json_string)\n",
    "        return True\n",
    "    except:\n",
    "        print(\"SyntaxError\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check_elements(): Checks whether the elements that will be processed in the analysis are contained or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_elements(json_string):\n",
    "    if json_string.find(\"\\\", \\\"sessionId\\\":\\\"\")>=0:\n",
    "        if json_string.find(\"\\\", \\\"robotName\\\":\\\"\") >=0:\n",
    "            if json_string.find(\"\\\", \\\"logLevel\\\":\\\"\")>=0:\n",
    "                if json_string.find(\"\\\", \\\"message\\\":\")>=0:\n",
    "                    return True\n",
    "    print(\"Error: Element is missing or broken\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validate_line():Cutting of data around the json data and checking it by using the other checker functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_line(line):\n",
    "    data_group = re.search(\"({.*})\",line)\n",
    "    try:\n",
    "        json_string = data_group.group()\n",
    "    except:\n",
    "        return False, \"\"\n",
    "    if data_group and check_bracket(json_string) and check_syntax(json_string) and check_elements(json_string):\n",
    "        json_dict = json.loads(data_group.group())\n",
    "        return True, json_dict\n",
    "    return False, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Calculation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the functions that are about calculating differences in time or using the time for something else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1: calculates the difference in time by using timedelta library and returns True if the difference is >= 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def more_than_2_days(date_two, date_one):\n",
    "    datetimeFormat = '%Y-%m-%d %H:%M:%S,%f'\n",
    "    diff = datetime.datetime.strptime(date_one, datetimeFormat)\\\n",
    "    - datetime.datetime.strptime(date_two, datetimeFormat)\n",
    "    if diff.days >=2:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_time_difference(date_two,date_one): returns the difference in minutes using timedelta library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_difference(date_two, date_one):\n",
    "    datetimeFormat = '%Y-%m-%d %H:%M:%S,%f'\n",
    "    diff = datetime.datetime.strptime(date_one, datetimeFormat)\\\n",
    "    - datetime.datetime.strptime(date_two, datetimeFormat)\n",
    "    return (diff.days/(24*60*60)+diff.seconds)/60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average_stay(dicti_user): takes an dictionary as input. The dictionary has a user_id (session_id + day of activity) as a key and contains a list of all server logs of this user as value.  By calculating the difference from the first and last log a time span is calculated. By summing up those for all the users dividing them by number of users the average is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_stay(dicti_user):\n",
    "    time_sum = 0\n",
    "    user = 0\n",
    "    for x in dicti_user:\n",
    "        if len(dicti_user[x])>=2:\n",
    "            time_sum =time_sum + get_time_difference(dicti_user[x][0][\"time\"],dicti_user[x][-1][\"time\"])\n",
    "            user = user + 1\n",
    "    if user >= 1:\n",
    "        return (time_sum / user)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "date_later(date_expect_later,date_expect_earlier): checks whether a given date is later than another one or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def date_later(date_expect_later,date_expect_earlier):\n",
    "    datetimeFormat = '%Y-%m-%d %H:%M:%S,%f'\n",
    "    return datetime.datetime.strptime(date_expect_later, datetimeFormat)>=datetime.datetime.strptime(date_expect_earlier, datetimeFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similarity_ratio(): Returns a value ( from 0 to 1) of the similarity of 2 strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_ratio(string_a, string_b, value):\n",
    "    return SequenceMatcher(None, string_a, string_b).ratio()>=value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "append_or_new_list(element, key, dicti): checks whether the given key is already in the dictionary or not. If it is than the element will be appended to the list which is the value of the given key. Otherwise a new list will be created for the given key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_or_new_list(element, key, dicti):\n",
    "    if key not in dicti:\n",
    "        dicti[key]=[]\n",
    "    dicti[key].append(element)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reduce_list_unique_elements(liste): takes a list as an input and reduces the list to another list, which only contains unique elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_list_unique_elements(liste):\n",
    "    new_set = {0,1}\n",
    "    return_list =[]\n",
    "    for x in liste:\n",
    "        if x not in new_set:\n",
    "            new_set.add(x)\n",
    "            return_list.append(x)\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading of every usefull piece of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reading_data_for_given_period(name,ftype,dstart,dend,checker): Reads in every line of a given file, which took place in a given period of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1 opens the given file to read and does an optional check of validation of the lines. The checker does usually double the time needed for reading in the files. \n",
    "#2 first sorts the list by the given time stamps and then only saves the actions, that took place in the given period by scanning through the list and check whether the date is later or not. If dend and dstart are equal to zero every piece of data will be processed. Otherwise only the data in the given range of time are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def reading_data_for_given_period(name,ftype,dstart,dend,checker):\n",
    "    json_list = []\n",
    "    start = time.time()\n",
    "    \n",
    "    #1\n",
    "    with open((name+ftype),\"rt\") as data_file: \n",
    "        if checker:\n",
    "            for line in data_file:\n",
    "                val_line=validate_line(line)\n",
    "                if val_line[0]:\n",
    "                    json_list.append(validate_line(line)[1])\n",
    "        else:\n",
    "            for line in data_file:\n",
    "                try:\n",
    "                    a=re.search(\"({.*})\",line).group()\n",
    "                    json_list.append(json.loads(a))\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "    end = time.time()\n",
    "    print(end-start, \"checker\")\n",
    "    \n",
    "    #2\n",
    "    json_list = sorted(json_list, key = lambda i: i['time'])\n",
    "    begin,end = 0,0\n",
    "    if dstart != 0 and dend !=0:\n",
    "        b_not_found = True\n",
    "        if date_later(dend,dstart):\n",
    "            for counter, element in enumerate(json_list):\n",
    "                if date_later(element[\"time\"],dstart) and b_not_found:\n",
    "                    print(\"ja\")\n",
    "                    begin=counter\n",
    "                    b_not_found = False\n",
    "                if date_later(element[\"time\"],dend):\n",
    "                    print(\"ne\")\n",
    "                    end = counter\n",
    "                    break\n",
    "                    json_list.append(0)\n",
    "                    return json_list[begin:end]\n",
    "        else:\n",
    "            print(\"Check entered dates. Try switch them\")\n",
    "    else:\n",
    "        print(\"No date entered. I will use everything\")\n",
    "    \n",
    "    return json_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaning through the logs and create a list of all actions, which is needed to assign the actions to the robots (#2), as well as creating an dictionary containing all the error messages provided in the logs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_input_list_actions(input_list):\n",
    "    stat_error_dict = {}\n",
    "    list_actions=[]\n",
    "    for element in input_list:\n",
    "        #1\n",
    "        if not \"action\" in element[\"message\"]:\n",
    "            append_or_new_list(element, element[\"message\"],stat_error_dict)\n",
    "        else: \n",
    "            list_actions.append(element[\"message\"][\"action\"])\n",
    "    return reduce_list_unique_elements(list_actions),stat_error_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scan_robot_names_userid_country_code(input_list,comp_list_actions): \n",
    "#1: scans through the whole list of statistic logs and creates a dict (robotName as key) which contains another dict as value. This dict uses the action as key and has a list as value, which contains all statistic logs with this robotName and this action. \n",
    "#2: scans through the whole list and checks whether the sessionId is known or not. Therefore it creates a userId by connecting the sessionId and the day the action took place. To make sure only unique user are counted and server restarts, which lead to a new count of sessionIds beginning from 1, are recognized the program uses the 2 day online approximation. This approximation starts to count user twice if they stay online for more than 2 days and perform at least one action after the 2 days.\n",
    "A small error takes place: If there was no server restart in June 2019 this leads to a difference of 1.5 percent of the user count. \n",
    "#3:\n",
    "creates a dict with the countrycode as a key and appends every statistic log to a list, which is the value of the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_robot_names_userid_country_code(input_list,comp_list_actions):\n",
    "    list_session_id={}\n",
    "    country_codes_dict={}\n",
    "    robot_names =[]\n",
    "    stat_dict ={}\n",
    "    robot_user_count = {}\n",
    "    #1\n",
    "    for element in input_list:   \n",
    "        if element[\"robotName\"] not in robot_names:\n",
    "            robot_user_count[element[\"robotName\"]]=[]\n",
    "            robot_names.append(element[\"robotName\"])\n",
    "            stat_dict[element[\"robotName\"]]={}\n",
    "            for action in comp_list_actions:\n",
    "                stat_dict[element[\"robotName\"]][action]=[]\n",
    "            stat_dict[element[\"robotName\"]][\"other\"]=[] \n",
    "        try:\n",
    "            stat_dict[element[\"robotName\"]][element[\"message\"][\"action\"]].append(element)\n",
    "        except:\n",
    "            stat_dict[element[\"robotName\"]][\"other\"].append(element)\n",
    "\n",
    "        #2\n",
    "\n",
    "        s_id = str(element[\"sessionId\"])+element[\"time\"][8:10]\n",
    "        case_a =s_id not in list_session_id\n",
    "        case_b = str(int(s_id)-1) not in list_session_id\n",
    "        case_c = str(int(s_id)-2) not in list_session_id\n",
    "        s_id = str(s_id)\n",
    "        if case_a and case_b and case_c:\n",
    "            list_session_id[s_id]= [element]\n",
    "\n",
    "        elif not case_a:\n",
    "            list_session_id[s_id].append(element)\n",
    "        elif not case_b:\n",
    "            list_session_id[str(int(s_id)-1)].append(element)\n",
    "        elif not case_c:\n",
    "            if more_than_2_days(list_session_id[str(int(s_id)-2)][0][\"time\"],element[\"time\"]):\n",
    "                list_session_id[s_id]=[element]\n",
    "\n",
    "            else:\n",
    "                list_session_id[str(int(s_id)-2)].append(element)\n",
    "        #3\n",
    "        try:\n",
    "            append_or_new_list(element, element[\"message\"][\"args\"][0][\"CountryCode\"], country_codes_dict)   \n",
    "        except:\n",
    "            pass\n",
    "    return list_session_id, country_codes_dict, robot_names, stat_dict, robot_user_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create_robot_specific_user_num(user_id_list,robot_user_count):\n",
    "counts every robot the user used once and add it to a dict, which contains the robotName as key and the users as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_robot_specific_user_num(user_id_list,robot_user_count):\n",
    "    for element in user_id_list:\n",
    "        robot_name_set={0,1}\n",
    "        for i in user_id_list[element]:\n",
    "            if \"action\" in i[\"message\"]:\n",
    "                if not i[\"robotName\"] in robot_name_set and i[\"message\"][\"action\"] != \"Initialization\":\n",
    "                    robot_user_count[i[\"robotName\"]].append(1)\n",
    "                    robot_name_set.add(i[\"robotName\"])\n",
    "            \n",
    "    for x in robot_user_count:\n",
    "        robot_user_count[x]= len(robot_user_count[x])\n",
    "    return robot_user_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filter_errors_and_sort(error_dict): uses an error_dictonary to get every error message and tries to find similarities to connect error message which contain a unique key, but are the same errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_errors_and_sort(error_dict):\n",
    "    comp_stat_error_dict = {}\n",
    "    error_messages_list = []\n",
    "    t = True\n",
    "    for x in error_dict:\n",
    "        if len(error_messages_list)>0:\n",
    "            for y in error_messages_list:\n",
    "                t=True\n",
    "                if similarity_ratio(y[0],x,0.85):\n",
    "                    y.append(x)\n",
    "                    t=False\n",
    "                    break\n",
    "            if t:\n",
    "                error_messages_list.append([x])              \n",
    "        else:\n",
    "            error_messages_list.append([x])\n",
    "\n",
    "    for x in error_messages_list:\n",
    "        comp_stat_error_dict[x[0]]=[]\n",
    "        for y in x:\n",
    "            comp_stat_error_dict[x[0]].extend(error_dict[y])\n",
    "    return comp_stat_error_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "country_codes_sort_robot(country_codes_dict): sorts the elements for a given country by creating a new dict as a value for the country code, which has the robot name as key and the corresponding elements in a list as value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def country_codes_sort_robot(country_codes_dict):\n",
    "    for country_code in country_codes_dict:\n",
    "        c_dict={}\n",
    "        for i in country_codes_dict[country_code]:\n",
    "            append_or_new_list(i, i[\"robotName\"], c_dict)\n",
    "        country_codes_dict[country_code]=c_dict\n",
    "    return country_codes_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writes the csv file containing all the information. create_nor_stat influences which data will be written into the file. Explanation above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_normal_stat(create_nor_stat, start,end,name,comp_list_actions,list_session_id,robot_user_count,stat_dict,country_codes_dict,robot_names):\n",
    "    with open(name+'_stat_analysis.csv','w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"durchschnittliche Verweildauer: \", average_stay(list_session_id),\"Zeitraum von: \",start,\"bis: \",end])\n",
    "        writer.writerow([\"Gesamtnutzer:\", len(list_session_id)])\n",
    "        writer.writerow([\"\"]+robot_names)\n",
    "        robot_user_count_list=[\"User\"]\n",
    "        for x in robot_user_count:\n",
    "            robot_user_count_list.append(robot_user_count[x])\n",
    "        writer.writerow(robot_user_count_list)\n",
    "        if create_nor_stat[1]:   \n",
    "            writer.writerow([])\n",
    "            for action in comp_list_actions:\n",
    "                action_count_list = []\n",
    "                action_count_list.append(action)\n",
    "                for i in robot_names:\n",
    "                    action_count_list.append(len(stat_dict[i][action]))\n",
    "                writer.writerow(action_count_list)\n",
    "        if create_nor_stat[2]:\n",
    "            writer.writerow([])\n",
    "            for country_code in country_codes_dict:\n",
    "                country_write_list = []\n",
    "                country_write_list.append(country_code)\n",
    "                for i in robot_names:\n",
    "                    if i in country_codes_dict[country_code]:\n",
    "                        country_write_list.append(len(country_codes_dict[country_code][i]))\n",
    "                    else:\n",
    "                        country_write_list.append(0)\n",
    "                writer.writerow(country_write_list)\n",
    "\n",
    "        \n",
    "        if create_nor_stat[3]:\n",
    "            writer.writerow([])\n",
    "            writer.writerow([\"UserId\",\"Uhrzeit\",\"Roboter Name\",\"Action/Fehler\",\"CountryCode\"])\n",
    "\n",
    "            for element in list_session_id:\n",
    "                for i in list_session_id[element]:\n",
    "                    liste = [element]\n",
    "                    liste.append(i[\"time\"])\n",
    "                    liste.append(i[\"robotName\"])\n",
    "                    if \"action\" in i[\"message\"]:\n",
    "                        liste.append(i[\"message\"][\"action\"])\n",
    "                    else:\n",
    "                        liste.append(i[\"message\"])\n",
    "                    if \"action\" in i[\"message\"]:\n",
    "                        if i[\"message\"][\"action\"] == \"Initialization\":\n",
    "                            if \"CountryCode\" in i[\"message\"][\"args\"][0]:\n",
    "                                liste.append(i[\"message\"][\"args\"][0][\"CountryCode\"])\n",
    "                    writer.writerow(liste)\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writes a csv file containing all information regarding error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_error_stat(robot_names,comp_stat_error_dict,name):\n",
    "    err_ticket_robot = {}\n",
    "    with open(name+'_stat_error_analysis.csv','w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow([\"\"]+robot_names)\n",
    "        for x in comp_stat_error_dict:\n",
    "            error_list = [x]\n",
    "            for name in robot_names:\n",
    "                count = 0\n",
    "                for i in comp_stat_error_dict[x]:\n",
    "                    if i[\"robotName\"]==name:\n",
    "                        count = count +1\n",
    "                error_list.append(count)\n",
    "            writer.writerow(error_list)\n",
    "        for x in comp_stat_error_dict:\n",
    "            if similarity_ratio(x,\"Exception. Error ticket: E-\",0.7):\n",
    "                for i in comp_stat_error_dict[x]:\n",
    "\n",
    "                    append_or_new_list(re.sub(\"\\D\", \"\", i[\"message\"]),i[\"robotName\"],err_ticket_robot)\n",
    "        #print(err_ticket_robot)\n",
    "        fin_list=[\"\"]\n",
    "        for name in robot_names:\n",
    "            if name in err_ticket_robot:\n",
    "                fin_list.append(','.join(err_ticket_robot[name]))\n",
    "            else:\n",
    "                fin_list.append(\"\")\n",
    "        writer.writerow(fin_list)    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name,ftype,create_err_stat,create_nor_stat,checker,start=0,end=0):\n",
    "    json_list = reading_data_for_given_period(name,ftype,start,end,checker)\n",
    "    comp_list_actions,stat_error_dict = scan_input_list_actions(json_list)\n",
    "    list_session_id, country_codes_dict, robot_names, stat_dict, robot_user_count = scan_robot_names_userid_country_code(json_list,comp_list_actions)\n",
    "    robot_user_count_fin = create_robot_specific_user_num(list_session_id,robot_user_count)\n",
    "    comp_stat_error_dict = filter_errors_and_sort(stat_error_dict)\n",
    "    country_codes_dict_sorted = country_codes_sort_robot(country_codes_dict)\n",
    "    if create_nor_stat[0]:\n",
    "        write_csv_normal_stat(create_nor_stat,start,end,name,comp_list_actions,list_session_id,robot_user_count,stat_dict,country_codes_dict_sorted, robot_names)\n",
    "    if create_err_stat:\n",
    "        write_csv_error_stat(robot_names,comp_stat_error_dict,name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Method is called with different parametres. Explanation can be found at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0019779205322265625 checker\n",
      "No date entered. I will use everything\n",
      "0.012212753295898438 gesamt\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "main(DATA_NAME,DATA_TYPE,ERR_STAT,NORMAL_STAT,True,0,0) #,DATA_START,DATA_END)\n",
    "end = time.time()\n",
    "print(end-start, \"gesamt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
